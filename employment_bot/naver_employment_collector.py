#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³ ìš©ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - ì¤‘ë³µ ì œê±° ì´ˆê°•í™” ë²„ì „
"""

import requests
from datetime import datetime, timedelta
from typing import List, Dict
import re

class NaverEmploymentCollector:
    """ê³ ìš©ë‰´ìŠ¤ ì „ë¬¸ ìˆ˜ì§‘ê¸° (ì¤‘ë³µ ì œê±° ì´ˆê°•í™”)"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        
        self.employment_keywords = [
            'ì±„ìš©', 'ì‹ ì…ì‚¬ì›', 'ê²½ë ¥ì§', 'êµ¬ì¸', 'ì¼ìë¦¬',
            'ì·¨ì—…', 'ê³ ìš©', 'ì¸ë ¥', 'ì§ì›ëª¨ì§‘', 'ë¦¬í¬ë£¨íŒ…',
            'ì…ì‚¬', 'ë©´ì ‘', 'ì¸ì¬ì±„ìš©', 'ëŒ€ê·œëª¨ì±„ìš©', 'ì²­ë…„ì±„ìš©'
        ]
    
    def collect_unique_news(self, count: int = 30) -> List[Dict]:
        """
        ì¤‘ë³µ ì œê±°ëœ ê³ ìš©ë‰´ìŠ¤ ìˆ˜ì§‘
        """
        
        all_news = []
        
        # í•µì‹¬ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        main_keywords = ['ì±„ìš© ê³µê³ ', 'ì‹ ì… ì±„ìš©', 'ëŒ€ê·œëª¨ ì±„ìš©', 'ì¼ìë¦¬', 'ì·¨ì—…']
        
        for keyword in main_keywords:
            try:
                news = self._search_news(keyword, display=15)
                all_news.extend(news)
            except Exception as e:
                print(f"âš ï¸ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"  ìˆ˜ì§‘: {len(all_news)}ê°œ")
        
        # 1ë‹¨ê³„: URL ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        unique_by_url = self._remove_duplicates_by_url(all_news)
        print(f"  URL ì¤‘ë³µ ì œê±° í›„: {len(unique_by_url)}ê°œ")
        
        # 2ë‹¨ê³„: ì œëª© í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ê°•í™”!)
        unique_by_title = self._remove_duplicates_by_title_v2(unique_by_url)
        print(f"  ì œëª© ì¤‘ë³µ ì œê±° í›„: {len(unique_by_title)}ê°œ")
        
        # 3ë‹¨ê³„: ë‚ ì§œ í•„í„°ë§
        filtered = self._filter_by_date(unique_by_title, days=2)
        print(f"  ë‚ ì§œ í•„í„°ë§ í›„: {len(filtered)}ê°œ")
        
        # 4ë‹¨ê³„: ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°
        scored = self._calculate_relevance_score(filtered)
        
        return scored[:count]
    
    def _search_news(self, query: str, display: int = 10) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ API ê²€ìƒ‰"""
        
        headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret
        }
        
        params = {
            'query': query,
            'display': display,
            'sort': 'date'
        }
        
        response = requests.get(
            self.base_url,
            headers=headers,
            params=params,
            timeout=10
        )
        
        if response.status_code == 200:
            return response.json().get('items', [])
        else:
            raise Exception(f"API ì˜¤ë¥˜: {response.status_code}")
    
    def _remove_duplicates_by_url(self, news_list: List[Dict]) -> List[Dict]:
        """URL ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
        
        seen_urls = set()
        unique_news = []
        
        for news in news_list:
            link = news.get('link', '')
            
            # URL ì •ê·œí™” (íŒŒë¼ë¯¸í„° ì œê±°)
            normalized_link = link.split('?')[0]
            
            if normalized_link and normalized_link not in seen_urls:
                seen_urls.add(normalized_link)
                unique_news.append(news)
        
        return unique_news
    
    def _remove_duplicates_by_title_v2(self, news_list: List[Dict]) -> List[Dict]:
        """
        ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì œê±° - ê°•í™” ë²„ì „
        í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ì‹ ê°œì„ 
        """
        
        seen_signatures = set()
        unique_news = []
        
        for news in news_list:
            title = self._clean_title(news.get('title', ''))
            
            # í•µì‹¬ í‚¤ì›Œë“œ ì‹œê·¸ë‹ˆì²˜ ìƒì„± (ê°œì„ !)
            signature = self._create_enhanced_signature(title)
            
            if signature and signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_news.append(news)
            else:
                # ë””ë²„ê·¸: ì¤‘ë³µ ì œê±°ëœ í•­ëª© ì¶œë ¥
                print(f"    ğŸ”„ ì¤‘ë³µ ì œê±°: {title[:30]}...")
        
        return unique_news
    
    def _create_enhanced_signature(self, title: str) -> str:
        """
        í–¥ìƒëœ ì‹œê·¸ë‹ˆì²˜ ìƒì„±
        
        í•µì‹¬ ì•„ì´ë””ì–´:
        1. ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 820ëª…, 200ëª…)
        2. íšŒì‚¬ëª… ì¶”ì¶œ (ì˜ˆ: ì„œìš¸êµí†µê³µì‚¬, í˜„ëŒ€ì¤‘ê³µì—…)
        3. í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ì±„ìš©, ìˆ˜ì£¼, íˆ¬ì)
        """
        
        # HTML íƒœê·¸ ì œê±°
        title = re.sub(r'<[^>]+>', '', title)
        title = title.replace('&quot;', '"').replace('&apos;', "'").replace('&amp;', '&')
        
        # 1. ìˆ«ì ì¶”ì¶œ (ì±„ìš© ì¸ì›, ê¸ˆì•¡ ë“±)
        numbers = re.findall(r'\d+(?:ë§Œ|ëª…|ì–µ|ì¡°)?', title)
        
        # 2. íšŒì‚¬ëª… ì¶”ì¶œ (ì£¼ìš” ê¸°ì—…ëª… íŒ¨í„´)
        companies = []
        company_keywords = [
            'í˜„ëŒ€', 'ì‚¼ì„±', 'ì—˜ì§€', 'LG', 'SK', 'í¬ìŠ¤ì½”', 'í•œí™”',
            'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤', 'ì¿ íŒ¡', 'ë°°ë¯¼', 'í† ìŠ¤',
            'êµí†µê³µì‚¬', 'ì „ë ¥ê³µì‚¬', 'ìˆ˜ìì›ê³µì‚¬', 'ë„ë¡œê³µì‚¬',
            'ê±´ì„¤', 'ì¤‘ê³µì—…', 'ì „ì', 'í™”í•™', 'ê¸ˆìœµ', 'ì€í–‰'
        ]
        
        for keyword in company_keywords:
            if keyword in title:
                companies.append(keyword)
        
        # 3. í•µì‹¬ í–‰ìœ„ ì¶”ì¶œ
        actions = []
        action_keywords = ['ì±„ìš©', 'ëª¨ì§‘', 'ì„ ë°œ', 'ì…ì‚¬', 'êµ¬ì¸', 'ìˆ˜ì£¼', 'íˆ¬ì', 'í™•ëŒ€', 'ì¦ì›']
        
        for keyword in action_keywords:
            if keyword in title:
                actions.append(keyword)
        
        # ì‹œê·¸ë‹ˆì²˜ ìƒì„±: íšŒì‚¬ëª… + ìˆ«ì + í–‰ìœ„
        signature_parts = []
        
        if companies:
            signature_parts.extend(sorted(companies)[:2])  # ìƒìœ„ 2ê°œ
        
        if numbers:
            signature_parts.extend(sorted(numbers)[:2])  # ìƒìœ„ 2ê°œ
        
        if actions:
            signature_parts.extend(sorted(actions)[:2])  # ìƒìœ„ 2ê°œ
        
        # ìµœì¢… ì‹œê·¸ë‹ˆì²˜
        signature = '_'.join(signature_parts)
        
        return signature.lower()
    
    def _clean_title(self, title: str) -> str:
        """ì œëª© ì •ë¦¬"""
        
        # HTML íƒœê·¸ ì œê±°
        title = re.sub(r'<[^>]+>', '', title)
        
        # HTML ì—”í‹°í‹° ë³€í™˜
        title = title.replace('&quot;', '"')
        title = title.replace('&apos;', "'")
        title = title.replace('&amp;', '&')
        
        return title.strip()
    
    def _filter_by_date(self, news_list: List[Dict], days: int = 2) -> List[Dict]:
        """ìµœê·¼ Nì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ"""
        
        cutoff_date = datetime.now() - timedelta(days=days)
        filtered = []
        
        for news in news_list:
            pub_date_str = news.get('pubDate', '')
            
            try:
                pub_date = datetime.strptime(
                    pub_date_str,
                    '%a, %d %b %Y %H:%M:%S %z'
                )
                
                pub_date_naive = pub_date.replace(tzinfo=None)
                
                if pub_date_naive >= cutoff_date:
                    filtered.append(news)
                    
            except Exception:
                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨
                filtered.append(news)
        
        return filtered
    
    def _calculate_relevance_score(self, news_list: List[Dict]) -> List[Dict]:
        """ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°"""
        
        for news in news_list:
            score = 0
            title = news.get('title', '').lower()
            description = news.get('description', '').lower()
            content = f"{title} {description}"
            
            # í•µì‹¬ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
            high_priority = ['ì±„ìš© ê³µê³ ', 'ì‹ ì… ì±„ìš©', 'ëŒ€ê·œëª¨ ì±„ìš©', 'ì¸ì¬ ì˜ì…']
            medium_priority = ['ì±„ìš©', 'êµ¬ì¸', 'ì¼ìë¦¬', 'ì…ì‚¬']
            
            for keyword in high_priority:
                if keyword in content:
                    score += 5
            
            for keyword in medium_priority:
                score += content.count(keyword) * 2
            
            for keyword in self.employment_keywords:
                if keyword in content:
                    score += 1
            
            news['relevance_score'] = score
        
        return sorted(
            news_list,
            key=lambda x: x.get('relevance_score', 0),
            reverse=True
        )
